import os
import tempfile
import gradio as gr
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

vectorstore = None

def build_knowledge_base(pdf_file):
    global vectorstore

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_file.read())
        tmp_path = tmp.name

    loader = PyPDFLoader(tmp_path)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

    return f"ì´ {len(chunks)}ê°œ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."


# ë¶„ì•¼ë³„ ë¼ìš°íŒ… ë° ì—ì´ì „íŠ¸ í•¨ìˆ˜ë“¤
def route_field(query):
    # LLMì„ í™œìš©í•œ ë¶„ì•¼ ë¶„ë¥˜
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY)
    prompt = (
        "ì•„ë˜ ì§ˆë¬¸ì´ ì–´ë–¤ êµ°ì‚¬ ë¶„ì•¼(ì‘ì „, êµ°ìˆ˜, íƒ„ì•½, ì˜ë¬´)ì— í•´ë‹¹í•˜ëŠ”ì§€ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. "
        "ë§Œì•½ í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ 'ì¼ë°˜'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
        f"ì§ˆë¬¸: {query}\n"
        "ë‹µë³€: "
    )
    response = llm.invoke(prompt)
    field = response.strip()
    if field not in ["ì‘ì „", "êµ°ìˆ˜", "íƒ„ì•½", "ì˜ë¬´"]:
        field = "ì¼ë°˜"
    return field

def operation_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì‘ì „")

def logistics_agent(query, vectorstore):
    return run_rag(query, vectorstore, "êµ°ìˆ˜")

def ammo_agent(query, vectorstore):
    return run_rag(query, vectorstore, "íƒ„ì•½")

def medical_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì˜ë¬´")

def default_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì¼ë°˜")

def run_rag(query, vectorstore, field):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY)
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
    result = qa({"query": f"[{field}] ë¶„ì•¼ ì§ˆë¬¸: {query}"})
    answer = result["result"]
    sources = "\n".join([f"- {doc.metadata.get('source','(PDF)')} p.{doc.metadata.get('page','?')}" for doc in result["source_documents"]])
    return f"### ğŸ’¬ ë‹µë³€\n{answer}\n\n---\nğŸ“– **ì¶œì²˜**\n{sources}"

def ask_question(query):
    global vectorstore
    if not vectorstore:
        return "âš ï¸ ë¨¼ì € êµë²” PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."

    # ë¶„ì•¼ íŒë³„ ë° ë¼ìš°íŒ…
    field = route_field(query)
    if field == "ì‘ì „":
        return operation_agent(query, vectorstore)
    elif field == "êµ°ìˆ˜":
        return logistics_agent(query, vectorstore)
    elif field == "íƒ„ì•½":
        return ammo_agent(query, vectorstore)
    elif field == "ì˜ë¬´":
        return medical_agent(query, vectorstore)
    else:
        return default_agent(query, vectorstore)


    # ë¶„ì•¼ë³„ ì—ì´ì „íŠ¸ ì˜ˆì‹œ (ì‹¤ì œ êµ¬í˜„ì€ ê° ë¶„ì•¼ë³„ íŠ¹í™” ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
def operation_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì‘ì „")

def logistics_agent(query, vectorstore):
    return run_rag(query, vectorstore, "êµ°ìˆ˜")

def ammo_agent(query, vectorstore):
    return run_rag(query, vectorstore, "íƒ„ì•½")

def medical_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì˜ë¬´")

def default_agent(query, vectorstore):
    return run_rag(query, vectorstore, "ì¼ë°˜")

def run_rag(query, vectorstore, field):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY)
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
    result = qa({"query": f"[{field}] ë¶„ì•¼ ì§ˆë¬¸: {query}"})
    answer = result["result"]
    sources = "\n".join([f"- {doc.metadata.get('source','(PDF)')} p.{doc.metadata.get('page','?')}" for doc in result["source_documents"]])
    return f"### ğŸ’¬ ë‹µë³€\n{answer}\n\n---\nğŸ“– **ì¶œì²˜**\n{sources}"

with gr.Blocks() as demo:
    gr.Markdown("# AITiger")

    with gr.Row():
        pdf_input = gr.File(label="ğŸ“„ êµë²” PDF ì—…ë¡œë“œ", file_types=[".pdf"])
        upload_btn = gr.Button("ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶•")

    status = gr.Textbox(label="ìƒíƒœ ë©”ì‹œì§€")

    query = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ë¹„ê°€ 4mm ì˜¤ëŠ” ìƒí™©ì—ì„œ, 10 km ì™¸ë¶€ ì§€ì—­ì— ì§€ì†ì„± ì‘ìš©ì œê°€ í¬íƒ„ìœ¼ë¡œ íˆ¬í•˜ì‹œ ëŒ€ì²˜í•´ì•¼ í•˜ëŠ”ê°€?")
    answer = gr.Markdown()

    upload_btn.click(build_knowledge_base, inputs=pdf_input, outputs=status)
    query.submit(ask_question, inputs=query, outputs=answer)

demo.launch(server_name="0.0.0.0", server_port=7860)
